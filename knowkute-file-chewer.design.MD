# Project Description

Knowkute File Chewer is a modular document processing and querying system powered by open-source Large Language Models (LLMs) and embeddings. It allows users to upload, parse, chunk, embed, and search across a wide variety of document types (PDF, Word, Excel, PowerPoint, and plain text) using a FastAPI-based backend. All processing and storage are performed locally, ensuring data privacy and full control over your files and models. The architecture is designed for extensibility, maintainability, and privacy, making it suitable for private document search, knowledge base construction, and secure enterprise use cases.

# Project Design: knowkute â€“ File Chewer

This document outlines the architecture and design for the `knowkute-file-chewer` project, a local-first RAG (Retrieval-Augmented Generation) system.

## Project Structure

knowkute-file-chewer/
â”‚â”€â”€ models/                         # Pre-downloaded local LLMs & Embedding Models
â”‚    â”œâ”€â”€ download_models.py         # Script to fetch all required models (LLaMA, embeddings, etc.)
â”‚    â””â”€â”€ <downloaded-model-folders> # HuggingFace / GGUF / Local model binaries
â”‚â”€â”€ app/
â”‚    â”‚â”€â”€ main.py                    # Entry point (FastAPI app startup, router mounting)
â”‚    â”‚â”€â”€ config.py                  # Centralized config using Pydantic's BaseSettings
â”‚    â”‚â”€â”€ __init__.py
â”‚
â”‚    â”œâ”€â”€ routes/                    # REST API endpoints
â”‚    â”‚    â”œâ”€â”€ __init__.py
â”‚    â”‚    â”œâ”€â”€ file_routes.py        # Upload & process file APIs
â”‚    â”‚    â”œâ”€â”€ query_routes.py       # Query APIs on processed chunks
â”‚    â”‚    â””â”€â”€ health_routes.py      # Healthcheck / status endpoints
â”‚
â”‚    â”œâ”€â”€ services/                  # Reusable processing & business logic
â”‚    â”‚    â”œâ”€â”€ __init__.py
â”‚    â”‚    â”œâ”€â”€ file_processor.py     # File parsing & chunking (PDF, Word, Excel, PPT, etc.)
â”‚    â”‚    â”œâ”€â”€ chunk_manager.py      # Sentence splitting, chunk creation
â”‚    â”‚    â”œâ”€â”€ embedding_service.py  # Handles embedding with llama_index + local models
â”‚    â”‚    â”œâ”€â”€ query_service.py      # Runs queries over stored vectors
â”‚    â”‚    â”œâ”€â”€ generation_service.py # Generates answers using a local LLM (Generation)
â”‚    â”‚    â””â”€â”€ storage_service.py    # Persists chunks/vectors locally (e.g., ChromaDB)
â”‚
â”‚    â”œâ”€â”€ schemas/                   # Python data models (Pydantic Schemas)
â”‚    â”‚    â”œâ”€â”€ __init__.py
â”‚    â”‚    â”œâ”€â”€ file_schema.py         # File metadata schema
â”‚    â”‚    â”œâ”€â”€ chunk_schema.py        # Chunk schema
â”‚    â”‚    â””â”€â”€ query_schema.py        # Query request/response schemas
â”‚
â”‚    â”œâ”€â”€ utils/                     # Shared utilities
â”‚    â”‚    â”œâ”€â”€ __init__.py
â”‚    â”‚    â”œâ”€â”€ logger.py             # Logging config
â”‚    â”‚    â”œâ”€â”€ file_utils.py         # File save/load helpers
â”‚    â”‚    â””â”€â”€ text_utils.py         # Text cleaning, sentence splitting helpers
â”‚
â”‚    â””â”€â”€ pipelines/                 # High-level orchestration
â”‚         â”œâ”€â”€ __init__.py
â”‚         â”œâ”€â”€ chew_pipeline.py      # Full "File Chewer" pipeline: parse -> chunk -> embed -> store
â”‚         â””â”€â”€ query_pipeline.py     # Full query pipeline: load embeddings â†’ query â†’ return result
â”‚
â”‚
â”‚â”€â”€ tests/                          # Unit & integration tests
â”‚    â”œâ”€â”€ __init__.py
â”‚    â”œâ”€â”€ test_file_processor.py
â”‚    â”œâ”€â”€ test_chunk_manager.py
â”‚    â”œâ”€â”€ test_embedding_service.py
â”‚    â”œâ”€â”€ test_query_service.py
â”‚    â””â”€â”€ test_endpoints.py
â”‚
â”‚â”€â”€ data/                           # Temporary files, storage
â”‚    â”œâ”€â”€ uploads/                   # Uploaded raw files
â”‚    â”œâ”€â”€ processed/                 # (Optional) Processed intermediate files
â”‚    â””â”€â”€ index_store/               # Persisted vector store (e.g., ChromaDB, FAISS)
â”‚
â”‚â”€â”€ requirements.txt                # Python dependencies
â”‚â”€â”€ README.md                       # Project documentation

---

## ðŸ”‘ Key Design Principles

*   **Asynchronous Processing**: File ingestion ("chewing") is a long-running operation. It will be executed as a background task using FastAPI's `BackgroundTasks` to avoid blocking the API and provide immediate feedback to the user.
*   **Dependency Injection**: The application will heavily leverage FastAPI's dependency injection system. This will be used to provide services, pipelines, and configuration settings to the API routes, promoting clean, decoupled, and easily testable code.
*   **Configuration Management**: All configuration (file paths, model names, chunking parameters) will be managed in `app/config.py` using Pydantic's `BaseSettings`. This allows for easy configuration via environment variables, `.env` files, or code defaults.
*   **Modularity and Separation of Concerns**: The structure separates API definitions (`routes`), business logic (`services`), high-level workflows (`pipelines`), and data structures (`schemas`), making the codebase easier to understand, maintain, and extend.

---

## ðŸ§© Component Responsibilities

#### 1. Schemas (`app/schemas/`)
*   **`file_schema.py`** â†’ Defines the schema for file metadata, including `id`, `name`, `path`, and a `status` enum (e.g., `UPLOADED`, `PROCESSING`, `COMPLETED`, `FAILED`).
*   **`chunk_schema.py`** â†’ Schema for file chunks, including `id`, `file_id`, `text`, and a reference to its embedding.
*   **`query_schema.py`** â†’ Defines the request and response formats for queries.

#### 2. Routes (`app/routes/`)
*   **`file_routes.py`** â†’ Endpoints for:
    *   `POST /files/upload`: Upload a file. Returns a file ID and triggers the chewing pipeline in the background.
    *   `GET /files/{file_id}/status`: Retrieve the processing status of a file.
*   **`query_routes.py`** â†’ `POST /query`: Endpoint for submitting a query against the processed files and receiving a generated answer.
*   **`health_routes.py`** â†’ Health check endpoints like `/health` to verify service status and model availability.

#### 3. Services (`app/services/`)
*   **`file_processor.py`** â†’ Detects file type (PDF, DOCX, PPTX, XLSX, TXT) and uses appropriate libraries to extract raw text.
*   **`chunk_manager.py`** â†’ Uses helpers from `utils.text_utils.py` to clean text, split it into sentences, and group them into chunks of a configurable size with overlap.
*   **`embedding_service.py`** â†’ Loads the local sentence-transformer model and generates vector embeddings for text chunks using `llama-index`.
*   **`query_service.py`** â†’ Executes a vector similarity search against the stored embeddings to find the most relevant context chunks for a given query.
*   **`generation_service.py`** â†’ Loads the local generative LLM (e.g., LLaMA GGUF) and uses it to synthesize a final, human-readable answer based on the user's query and the retrieved context chunks.
*   **`storage_service.py`** â†’ Manages the persistence layer. It configures and provides access to the `llama-index` `StorageContext`, which abstracts the underlying vector store (e.g., ChromaDB, FAISS) located in `/data/index_store/`.

#### 4. Pipelines (`app/pipelines/`)
*   **`chew_pipeline.py`** â†’ Orchestrates the entire ingestion process: `File Processor` â†’ `Chunk Manager` â†’ `Embedding Service` â†’ `Storage Service`.
*   **`query_pipeline.py`** â†’ Orchestrates the full RAG query flow: Embeds the user query, uses `Query Service` to retrieve context, uses `Generation Service` to create a final answer, and returns the response.

#### 5. Utils (`app/utils/`)
*   **`logger.py`** â†’ Configurable structured logging setup for the application.
*   **`file_utils.py`** â†’ Helper functions for safely saving and managing uploaded files.
*   **`text_utils.py`** â†’ Pure, stateless functions for text manipulation like cleaning special characters and splitting text into sentences.

#### 6. Models Directory (`models/`)
*   Stores all local LLM and embedding models to keep them separate from application code.
*   Includes a script `download_models.py` that uses the `huggingface_hub` library to programmatically fetch and save required models from Hugging Face.
*   Example:
    ```
    models/
        ggml-llama-7b.bin
        all-MiniLM-L6-v2/
        download_models.py
    ```

---

## ï¿½ Workflow

#### 1. File Ingestion (The "Chew" Pipeline)
1.  **Upload**: A user sends a `POST` request to `/files/upload` with a file.
2.  **Save & Enqueue**: The file is saved to `/data/uploads/`. A new file record is created in the database/metadata store with a `status` of `UPLOADED`. The `chew_pipeline` is triggered as a background task.
3.  **Process**: The pipeline updates the file `status` to `PROCESSING`.
    *   **Parse**: `file_processor.py` extracts raw text from the file.
    *   **Chunk**: `chunk_manager.py` cleans and splits the text into semantic chunks.
    *   **Embed**: `embedding_service.py` converts each chunk into a vector embedding.
    *   **Store**: `storage_service.py` persists the chunks and their embeddings into the vector store in `/data/index_store/`.
4.  **Complete**: The file `status` is updated to `COMPLETED`. If any step fails, it's marked as `FAILED` with an error message.

#### 2. Querying (The RAG Pipeline)
1.  **Query**: A user sends a `POST` request to `/query` with a question.
2.  **Retrieve**: The `query_pipeline` is triggered.
    *   The user's query is embedded using the same `embedding_service`.
    *   `query_service` performs a vector similarity search in the index to find the top-N most relevant text chunks.
3.  **Generate**:
    *   The retrieved chunks (context) and the original query are passed to the `generation_service`.
    *   The local LLM generates a synthesized answer based on the provided context.
4.  **Respond**: The generated answer and source chunks are returned to the user.

---

ðŸ‘‰ **This design ensures:**
*   **Reusability** â†’ Services and pipelines are modular and can be composed differently.
*   **Extensibility** â†’ Easy to add support for new file types, vector stores, or LLMs by adding or replacing service implementations.
*   **Local-First** â†’ All models and data remain on the local machine, ensuring privacy and offline capability.
*   **Clear Separation of Concerns** â†’ Routes (API), Services (logic), Pipelines (orchestration), and Schemas (data) are cleanly separated.
*   **Scalability** â†’ The use of background tasks for ingestion allows the API to remain responsive under load.
